# ============================================================================
# Scheduled PR Data Refresh (Merge-and-Replace)
# ============================================================================
# This workflow fetches recent PR data, merges it with the existing dataset
# (deduplicating by pr_number + repository), and uploads the result to
# cloud storage.
#
# Flow:
#   1. Download existing CSV from cloud storage (if it exists)
#   2. Fetch new PRs from the last 30 days (or custom date range)
#   3. Merge: new data overwrites existing records (upsert)
#   4. Upload the deduplicated CSV back to cloud storage
#   5. Save the CSV as a workflow artifact for auditing
#
# Setup:
#   1. Add these secrets to your repository (Settings → Secrets → Actions):
#      - GITHUB_TOKEN_PAT: GitHub PAT with 'repo' scope
#      - AZURE_STORAGE_CONNECTION_STRING: (optional) Azure Blob Storage
#      - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY + AWS_S3_BUCKET: (optional) S3
#
#   2. Set STORAGE_PROVIDER to 'azure', 's3', or 'none'
#
#   3. Update DEFAULT_OWNER with your org name
#
#   4. Enable the workflow (Actions → refresh-data → Enable workflow)
# ============================================================================

name: Refresh PR Data

on:
  # Run weekly on Monday at 6am UTC
  schedule:
    - cron: "0 6 * * 1"

  # Allow manual trigger from the Actions tab
  workflow_dispatch:
    inputs:
      owner:
        description: "GitHub organization name"
        required: true
      start_date:
        description: "Start date filter (YYYY-MM-DD, optional)"
        required: false
      end_date:
        description: "End date filter (YYYY-MM-DD, optional)"
        required: false
      full_refresh:
        description: "Full refresh (ignore existing data)"
        type: boolean
        default: false

env:
  # Default organization to scan — override via workflow_dispatch inputs
  DEFAULT_OWNER: "your-org-name"
  PYTHON_VERSION: "3.12"
  # Set to 'azure', 's3', or 'none' for artifact-only
  STORAGE_PROVIDER: "none"

jobs:
  fetch-pr-data:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          if [ "${{ env.STORAGE_PROVIDER }}" = "azure" ]; then
            pip install azure-storage-blob
          elif [ "${{ env.STORAGE_PROVIDER }}" = "s3" ]; then
            pip install boto3
          fi

      - name: Determine parameters
        id: params
        env:
          INPUT_OWNER: ${{ github.event.inputs.owner }}
          INPUT_START_DATE: ${{ github.event.inputs.start_date }}
          INPUT_END_DATE: ${{ github.event.inputs.end_date }}
          INPUT_FULL_REFRESH: ${{ github.event.inputs.full_refresh }}
          EVENT_NAME: ${{ github.event_name }}
        run: |
          # Use workflow_dispatch inputs if available, otherwise defaults
          OWNER="${INPUT_OWNER:-$DEFAULT_OWNER}"
          echo "owner=$OWNER" >> $GITHUB_OUTPUT

          # For scheduled runs, default to last 30 days for incremental fetch
          if [ -n "$INPUT_START_DATE" ]; then
            echo "start_date=--start-date $INPUT_START_DATE" >> $GITHUB_OUTPUT
          elif [ "$EVENT_NAME" = "schedule" ]; then
            START=$(date -d '-30 days' '+%Y-%m-%d' 2>/dev/null || date -v-30d '+%Y-%m-%d')
            echo "start_date=--start-date $START" >> $GITHUB_OUTPUT
          else
            echo "start_date=" >> $GITHUB_OUTPUT
          fi

          if [ -n "$INPUT_END_DATE" ]; then
            echo "end_date=--end-date $INPUT_END_DATE" >> $GITHUB_OUTPUT
          else
            echo "end_date=" >> $GITHUB_OUTPUT
          fi

          # Determine if we should merge (not a full refresh)
          if [ "$INPUT_FULL_REFRESH" = "true" ]; then
            echo "merge_flags=" >> $GITHUB_OUTPUT
          else
            echo "merge_flags=--merge --snapshot" >> $GITHUB_OUTPUT
          fi

      - name: Download existing data from cloud storage
        if: env.STORAGE_PROVIDER != 'none' && github.event.inputs.full_refresh != 'true'
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        run: |
          python src/upload_data.py \
            --provider "$STORAGE_PROVIDER" \
            --download \
            --file data/pull_requests.csv

      - name: Fetch and merge PR data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN_PAT }}
        run: |
          python src/fetch_pr_data.py \
            --owner "${{ steps.params.outputs.owner }}" \
            ${{ steps.params.outputs.start_date }} \
            ${{ steps.params.outputs.end_date }} \
            ${{ steps.params.outputs.merge_flags }} \
            --output data/pull_requests.csv

      - name: Upload to cloud storage
        if: env.STORAGE_PROVIDER != 'none'
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        run: |
          python src/upload_data.py \
            --provider "$STORAGE_PROVIDER" \
            --file data/pull_requests.csv

      - name: Upload CSV as artifact
        uses: actions/upload-artifact@v4
        with:
          name: pr-data-${{ github.run_id }}
          path: data/pull_requests.csv
          retention-days: 90
